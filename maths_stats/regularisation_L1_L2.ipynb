{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "## Introduction\n",
    "- General topic: model complexity\n",
    "- Regularisation help generalise model/reduce complexity and reduce overfitting\n",
    " - overfitting model = too complex\n",
    " - 'channel your inner Ockham' \n",
    "  - minimise loss + penalise complexity: \n",
    "  - minimise(Loss(Data,Model) + complexity(Model))\n",
    "- Components of model complexity\n",
    " - Model complexity as a function of the weights of all the features in the model.\n",
    "  - If model complexity is a function of weights, a feature weight with a high absolute value is more complex than a feature weight with a low absolute value.\n",
    " - Model complexity as a function of the total number of features with nonzero weights. (A later module covers this approach.)\n",
    "- The best way to understand regularization is to see the implications it has on our loss function.\n",
    " - In mathematical optimization and decision theory, a [loss function](https://en.wikipedia.org/wiki/Loss_function) or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of regularization techniques\n",
    "- L2 regularisation - takes in sum of square\n",
    "- L1 regularisation - takes in absolute value\n",
    "- Elastic Net - combine both L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| L2   | L1   |\n",
    "|------|------|\n",
    "| square  | absolute|\n",
    "| punished large numbers  | affect numbers equally|\n",
    "| 'really wants small values in the whole matrix'  | 'doesn't care if we put all the large value in a single slot in the matrix'|\n",
    "| spreads error throughout the weight matrix  | sparse weight matrix (some exactly zero, some relatively large)|\n",
    "| takes in all component of the weight matrix  | encourage many of the uninformative coefficients in our model to be exactly 0|\n",
    "|    | can save RAM and may reduce noise in the mode|\n",
    "\n",
    "-  _The former case is sufficient and indeed suitable for a variety of statistical problems, but the latter is gaining traction through the field of compressive sensing.  From a non-rigorous standpoint, compressive sensing assumes not that observations come from Gaussian-distributed sources about ground truth but rather that sparse or simple solutions to equations are preferable or more likely (the \"Occam's Razor\" approach)._ - from Justin Solomon's answer in Quora\n",
    "- _For example, consider a housing data set that covers not just California but the entire globe. Bucketing global latitude at the minute level (60 minutes per degree) gives about 10,000 dimensions in a sparse encoding; global longitude at the minute level gives about 20,000 dimensions. A feature cross of these two features would result in roughly 200,000,000 dimensions. Many of those 200,000,000 dimensions represent areas of such limited residence (for example, the middle of the ocean) that it would be difficult to use that data to generalize effectively. It would be silly to pay the RAM cost of storing these unneeded dimensions. Therefore, it would be nice to encourage the weights for the meaningless dimensions to drop to exactly 0, which would allow us to avoid paying for the storage cost of these model coefficients at inference time._ - great illustration for the motivation for L1 (from Google Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 comparison \n",
    "### Note - show when is it useful to use L1 vs L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinecurve.com/index.php/2020/01/23/how-to-use-l1-l2-and-elastic-net-regularization-with-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 421.8 MB 73 kB/s  eta 0:00:012     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 129.6 MB 2.2 MB/s eta 0:02:15‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 419.6 MB 1.5 MB/s eta 0:00:02\n",
      "\u001b[?25hProcessing /home/febriyan/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
      "Requirement already satisfied: six>=1.12.0 in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 32.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.8 MB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/febriyan/.local/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.27.2-cp37-cp37m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.7 MB 29.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Processing /home/febriyan/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6/termcolor-1.1.0-cp37-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow) (46.0.0.post20200311)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88 kB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 298 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/febriyan/anaconda3/envs/ml/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=aeb8d7553c508389c2b55720d5ef679ef2f61bca8d165c791dfcc3167e8a6e8d\n",
      "  Stored in directory: /home/febriyan/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built absl-py\n",
      "Installing collected packages: gast, opt-einsum, protobuf, astor, tensorflow-estimator, absl-py, markdown, oauthlib, idna, chardet, urllib3, requests, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, werkzeug, grpcio, tensorboard, google-pasta, h5py, keras-applications, keras-preprocessing, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 chardet-3.0.4 gast-0.2.2 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.27.2 h5py-2.10.0 idna-2.9 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.2.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 urllib3-1.25.8 werkzeug-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'extra_keras_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e8b0302cc34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mextra_keras_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'extra_keras_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from extra_keras_datasets import emnist\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model configuration\n",
    "img_width, img_height, num_channels = 28, 28, 1\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "batch_size = 250\n",
    "no_epochs = 25\n",
    "no_classes = 47\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# Load EMNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = emnist.load_data()\n",
    "\n",
    "# Add number of channels to EMNIST data\n",
    "input_train = input_train.reshape((len(input_train), img_height, img_width, num_channels))\n",
    "input_test  = input_test.reshape((len(input_test), img_height, img_width, num_channels))\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "\n",
    "# Convert target vectors to categorical targets\n",
    "target_train = tensorflow.keras.utils.to_categorical(target_train, no_classes)\n",
    "target_test = tensorflow.keras.utils.to_categorical(target_test, no_classes)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "model.add(Dense(no_classes, activation='softmax', activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(input_train, target_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(input_test, target_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "# Plot history: Loss\n",
    "plt.plot(history.history['loss'], label='Training data')\n",
    "plt.plot(history.history['val_loss'], label='Validation data')\n",
    "plt.title('L1/L2 Activity Loss')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Plot history: Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training data')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation data')\n",
    "plt.title('L1/L2 Activity Accuracy')\n",
    "plt.ylabel('%')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization\n",
    "- https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization\n",
    "- https://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/\n",
    "- https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b\n",
    "- https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when\n",
    "- https://www.machinecurve.com/index.php/2020/01/23/how-to-use-l1-l2-and-elastic-net-regularization-with-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Questions\n",
    "- What is regularization? Why do we need it? üë∂\n",
    "- Which regularization techniques do you know? üë©‚Äçüéì\n",
    "- What kind of regularization techniques are applicable to linear models? üë©‚Äçüéì\n",
    "- How does L2 regularization look like in a linear model? üë©‚Äçüéì\n",
    "- How do we select the right regularization parameters? üë∂\n",
    "- What‚Äôs the effect of L2 regularization on the weights of a linear model? üë©‚Äçüéì\n",
    "- How L1 regularization looks like in a linear model? üë©‚Äçüéì\n",
    "- What‚Äôs the difference between L2 and L1 regularization? üë©‚Äçüéì\n",
    "- Can we have both L1 and L2 regularization components in a linear model? üë©‚Äçüéì\n",
    "- How do we interpret weights in linear models? üë©‚Äçüéì\n",
    "- If a weight for one variable is higher than for another - can we say that this variable is more important? üë©‚Äçüéì\n",
    "- Can we use L1 regularization for feature selection? üë©‚Äçüéì\n",
    "- Can we use L2 regularization for feature selection? üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
